{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import wandb\n",
    "\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import trange\n",
    "from PIL import Image\n",
    "from torch.nn import PoissonNLLLoss, MSELoss\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from os.path import join\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from fix_models.feature_extractors import resnet3d18_reg, get_video_feature_extractor, resnet50_reg, get_image_feature_extractor, ImageFeatureExtractor, VideoFeatureExtractor, dorsalnet_reg\n",
    "from fix_models.datasets import VideoDataset, ImageDataset, get_datasets_and_loaders, get_search_dataset_and_loader\n",
    "from fix_models.transforms import BaseVideoTransform, BaseImageTransform\n",
    "from fix_models.readouts import PoissonGaussianReadout\n",
    "from fix_models.metrics import corr_to_avg\n",
    "from fix_models.models import FullModel\n",
    "\n",
    "from fix_models.feature_extractors import ImageFeatureExtractor, VideoFeatureExtractor\n",
    "from fix_models.readouts import PoissonGaussianReadout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip3 install pandas\\n!pip3 install scipy\\n!pip3 install imagehash\\n!pip3 install matplotlib\\n!pip3 install tqdm\\n!pip3 install wandb\\n!pip install python_dict_wrapper\\n!pip install GitPython\\n!pip install tables\\n!apt-get update && apt-get install ffmpeg libsm6 libxext6  -y\\n!pip install opencv-python\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!pip3 install pandas\n",
    "!pip3 install scipy\n",
    "!pip3 install imagehash\n",
    "!pip3 install matplotlib\n",
    "!pip3 install tqdm\n",
    "!pip3 install wandb\n",
    "!pip install python_dict_wrapper\n",
    "!pip install GitPython\n",
    "!pip install tables\n",
    "!apt-get update && apt-get install ffmpeg libsm6 libxext6  -y\n",
    "!pip install opencv-python\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all parameters\n",
    "config = dict()\n",
    "config[\"modality\"] = \"video\" # or image\n",
    "\n",
    "# paths\n",
    "input_dir = f'./data/{config[\"modality\"]}/'\n",
    "stimulus_dir = f'./data/{config[\"modality\"]}/stimuli/'\n",
    "embedding_dir = f'./data/{config[\"modality\"]}/embeddings/'\n",
    "model_output_path = f'./data/{config[\"modality\"]}/model_output/results'\n",
    "\n",
    "# image defaults\n",
    "if config[\"modality\"] == \"image\":\n",
    "    # model parameters\n",
    "    config[\"layer\"] = \"layer3\"\n",
    "    config['use_sigma'] = True\n",
    "    config['center_readout'] = False\n",
    "    config[\"use_pool\"] = True\n",
    "    config[\"pool_size\"] = 4\n",
    "    config[\"pool_stride\"] = 1\n",
    "    config[\"use_pretrained\"] = True\n",
    "\n",
    "    config[\"flatten_time\"] = True\n",
    "\n",
    "\n",
    "    # stimulus parameters \n",
    "    config[\"stim_size\"] = 32 #25 #[25, 50, 100]\n",
    "    config[\"win_size\"] = 240 #[50, 100, 180]\n",
    "    if config[\"modality\"] == \"video\":\n",
    "        stim_dur_ms = 200\n",
    "        stim_shape = (1, 3, 5, config[\"stim_size\"], config[\"stim_size\"])\n",
    "    elif config[\"modality\"] == \"image\":\n",
    "        stim_dur_ms = 120\n",
    "        stim_shape = (1, 3, config[\"stim_size\"], config[\"stim_size\"])\n",
    "    \n",
    "    # training parameters \n",
    "    config[\"exp_var_thresholds\"] = [0.1, 0.1, 0.1, 0.1] #[0.15, 0.15, 0.3, 0.25]\n",
    "    config[\"lr\"] = 0.001 #1 #[0.001, 0.01, 0.1]\n",
    "    config[\"batch_size\"] = 16\n",
    "    config[\"num_epochs\"] = 20#1\n",
    "    config[\"l2_weight\"] = 0#1#[0.01, 0.1, 1, 1]\n",
    "\n",
    "    config[\"feat_ext_type\"] = 'resnet50'\n",
    "\n",
    "# video defaults\n",
    "if config[\"modality\"] == \"video\":\n",
    "    # model parameters\n",
    "    config[\"layer\"] = \"layer1\"\n",
    "    config[\"use_sigma\"] = True\n",
    "    config[\"center_readout\"] = False\n",
    "    config[\"use_pool\"] = True\n",
    "    config[\"pool_size\"] = 4\n",
    "    config[\"pool_stride\"] = 2\n",
    "    config[\"use_pretrained\"] = True\n",
    "\n",
    "    config[\"flatten_time\"] = True\n",
    "\n",
    "    # stimulus parameters \n",
    "    config[\"win_size\"] = 240 #240# #240 #180 #180 #[50, 100, 180]\n",
    "\n",
    "    config[\"feat_ext_type\"] = 'dorsalnet'\n",
    "    config[\"stim_size\"] = 32 #32 #50 #25 #[25, 50, 100]\n",
    "    if config[\"feat_ext_type\"] == 'hiera':\n",
    "        config[\"stim_size\"] = 224 #50 #25 #[25, 50, 100]\n",
    "\n",
    "    if config[\"modality\"] == \"video\":\n",
    "        stim_dur_ms = 200\n",
    "        stim_shape = (1, 3, 5, config[\"stim_size\"], config[\"stim_size\"])\n",
    "    elif config[\"modality\"] == \"image\":\n",
    "        stim_dur_ms = 120\n",
    "        stim_shape = (1, 3, config[\"stim_size\"], config[\"stim_size\"])\n",
    "    \n",
    "    # training parameters \n",
    "    config[\"exp_var_thresholds\"] = [0.25, 0.25, 0.25] #[0.15, 0.15, 0.15] #[-1, -1, -1] #[0.15, 0.15, 0.15, 0.15]\n",
    "    config[\"lr\"] = 0.001 #1 #[0.001, 0.01, 0.1]\n",
    "    config[\"batch_size\"] = 16\n",
    "    config[\"num_epochs\"] = 20 #1\n",
    "    config[\"l2_weight\"] = 0 #1e-5 #0 #s1e-3# 0 #0 #0.01 #.001 #.001 #0.001 #0.001 # 0 #.001 #1#[0.01, 0.1, 1, 1]\n",
    "    config[\"first_frame_only\"] = False\n",
    "    config[\"blur_sigma\"] = 0\n",
    "\n",
    "    config[\"mlp\"] = False\n",
    "    config[\"loss\"] = \"poisson\"\n",
    "\n",
    "config['pos'] = (400, 180)\n",
    "\n",
    "# logging\n",
    "config[\"wandb\"] = True\n",
    "\n",
    "config[\"ensemble\"] = False\n",
    "\n",
    "# save model\n",
    "config[\"save\"] = True\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# session names\n",
    "if config[\"modality\"] == \"video\":\n",
    "    session_ids = [\"082824\", \"082924\", \"083024\"] #\"082824\", \n",
    "elif config[\"modality\"] == \"image\":\n",
    "    session_ids = [\"051724\", \"081624\", \"080124\", \"082324\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Train encoding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1g1fwx6c) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ancient-capybara-249</strong> at: <a href='https://wandb.ai/et22/video-basline/runs/1g1fwx6c' target=\"_blank\">https://wandb.ai/et22/video-basline/runs/1g1fwx6c</a><br/> View project at: <a href='https://wandb.ai/et22/video-basline' target=\"_blank\">https://wandb.ai/et22/video-basline</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241015_213352-1g1fwx6c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1g1fwx6c). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/src/models/wandb/run-20241015_213428-9ks5iup3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/et22/video-basline/runs/9ks5iup3' target=\"_blank\">wandering-sun-250</a></strong> to <a href='https://wandb.ai/et22/video-basline' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/et22/video-basline' target=\"_blank\">https://wandb.ai/et22/video-basline</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/et22/video-basline/runs/9ks5iup3' target=\"_blank\">https://wandb.ai/et22/video-basline/runs/9ks5iup3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readout input shape: torch.Size([1, 32, 5, 8, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/src/models/./baselines/dorsalnet/models.py:529: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=args.device)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/feature_extraction.py:174: UserWarning: NOTE: The nodes obtained by tracing the model in eval mode are a subsequence of those obtained in train mode. When choosing nodes for feature extraction, you may need to specify output nodes for train and eval mode separately.\n",
      "  warnings.warn(msg + suggestion_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch 1 loss: 0.1254706863709438 corr: 0.07201377483141577\n",
      " num. neurons : 54\n",
      "  epoch 2 loss: 0.11321003013186985 corr: 0.11567752897549415\n",
      " num. neurons : 54\n",
      "  epoch 3 loss: 0.11136853471214389 corr: 0.1425432886578258\n",
      " num. neurons : 54\n",
      "  epoch 4 loss: 0.11020525726271264 corr: 0.17389835658907163\n",
      " num. neurons : 54\n",
      "  epoch 5 loss: 0.10950821393801842 corr: 0.20020052010362915\n",
      " num. neurons : 54\n",
      "  epoch 6 loss: 0.10896263057802931 corr: 0.20969730408812776\n",
      " num. neurons : 54\n",
      "  epoch 7 loss: 0.1085477418664061 corr: 0.22898195640833502\n",
      " num. neurons : 54\n",
      "  epoch 8 loss: 0.10825363147405931 corr: 0.22834540440348441\n",
      " num. neurons : 54\n",
      "  epoch 9 loss: 0.10800979655465962 corr: 0.25268914358508443\n",
      " num. neurons : 54\n",
      "  epoch 10 loss: 0.10774150836614915 corr: 0.2630346954978146\n",
      " num. neurons : 54\n",
      "  epoch 11 loss: 0.10742415251555266 corr: 0.26333880462889897\n",
      " num. neurons : 54\n",
      "  epoch 12 loss: 0.10713574550769947 corr: 0.2699976575726546\n",
      " num. neurons : 54\n",
      "  epoch 13 loss: 0.10699078854219413 corr: 0.28720021638895926\n",
      " num. neurons : 54\n",
      "  epoch 14 loss: 0.10684057942143194 corr: 0.28736731113329345\n",
      " num. neurons : 54\n",
      "  epoch 15 loss: 0.10667014828434697 corr: 0.2828990453351749\n",
      " num. neurons : 54\n",
      "  epoch 16 loss: 0.10652248005808136 corr: 0.2836328294712568\n",
      " num. neurons : 54\n",
      "  epoch 17 loss: 0.10638894104663237 corr: 0.29304704850121593\n",
      " num. neurons : 54\n",
      "  epoch 18 loss: 0.10623694060761252 corr: 0.3029735530536326\n",
      " num. neurons : 54\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModelEnsembler(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super(ModelEnsembler, self).__init__()\n",
    "        self.models = nn.ModuleList(models)  # List of models\n",
    "        self.weights = nn.Parameter(torch.ones((4, 1, 1), device=device)/4)\n",
    "    def forward(self, x):\n",
    "        # Get the outputs of each model\n",
    "        outputs = [model(x) for model in self.models]\n",
    "        # Stack the outputs along a new dimension and average them\n",
    "        self.weights.data = torch.clamp(self.weights, 0, 1)\n",
    "        avg_output = torch.sum(self.weights/torch.sum(self.weights)*torch.stack(outputs), 0)\n",
    "        return avg_output\n",
    "        \n",
    "all_corrs = []\n",
    "xs = []\n",
    "\n",
    "#l2_weights = [0, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "#lrs = [1e-3, 1e-5, 1e-4, 1e-2, 1e-1]\n",
    "\n",
    "#config[\"win_size\"] = 180 #180 #180 #[50, 100, 180]\n",
    "\"\"\"\n",
    "for modality in [\"video\"]:#, \"image\"]:\n",
    "    # all parameters\n",
    "    config = dict()\n",
    "    config[\"modality\"] = modality # or image\n",
    "    \n",
    "    # paths\n",
    "    input_dir = f'./data/{config[\"modality\"]}/'\n",
    "    stimulus_dir = f'./data/{config[\"modality\"]}/stimuli/'\n",
    "    embedding_dir = f'./data/{config[\"modality\"]}/embeddings/'\n",
    "    model_output_path = f'./data/{config[\"modality\"]}/model_output/results'\n",
    "    \n",
    "    # image defaults\n",
    "    if config[\"modality\"] == \"image\":\n",
    "        # model parameters\n",
    "        config[\"layer\"] = \"layer3\"\n",
    "        config['use_sigma'] = True\n",
    "        config['center_readout'] = False\n",
    "        config[\"use_pool\"] = True\n",
    "        config[\"pool_size\"] = 2\n",
    "        config[\"pool_stride\"] = 1\n",
    "        config[\"use_pretrained\"] = True\n",
    "    \n",
    "        config[\"flatten_time\"] = False\n",
    "    \n",
    "    \n",
    "        # stimulus parameters \n",
    "        config[\"stim_size\"] = 32 #25 #[25, 50, 100]\n",
    "        config[\"win_size\"] = 240 #[50, 100, 180]\n",
    "        if config[\"modality\"] == \"video\":\n",
    "            stim_dur_ms = 200\n",
    "            stim_shape = (1, 3, 5, config[\"stim_size\"], config[\"stim_size\"])\n",
    "        elif config[\"modality\"] == \"image\":\n",
    "            stim_dur_ms = 120\n",
    "            stim_shape = (1, 3, config[\"stim_size\"], config[\"stim_size\"])\n",
    "        \n",
    "        # training parameters \n",
    "        config[\"exp_var_thresholds\"] = [0.1, 0.1, 0.1, 0.1] #[0.15, 0.15, 0.3, 0.25]\n",
    "        config[\"lr\"] = 0.001 #1 #[0.001, 0.01, 0.1]\n",
    "        config[\"batch_size\"] = 16\n",
    "        config[\"num_epochs\"] = 20#1\n",
    "        config[\"l2_weight\"] = 0#1#[0.01, 0.1, 1, 1]\n",
    "    \n",
    "        config[\"feat_ext_type\"] = 'resnet50'\n",
    "    \n",
    "    # video defaults\n",
    "    if config[\"modality\"] == \"video\":\n",
    "        # model parameters\n",
    "        config[\"layer\"] = \"layer3\"\n",
    "        config[\"use_sigma\"] = True\n",
    "        config[\"center_readout\"] = False\n",
    "        config[\"use_pool\"] = True\n",
    "        config[\"pool_size\"] = 2\n",
    "        config[\"pool_stride\"] = 1\n",
    "        config[\"use_pretrained\"] = True\n",
    "    \n",
    "        config[\"flatten_time\"] = True\n",
    "    \n",
    "        # stimulus parameters \n",
    "        config[\"stim_size\"] = 32 #50 #25 #[25, 50, 100]\n",
    "        config[\"win_size\"] = 240 #180 #180 #[50, 100, 180]\n",
    "        if config[\"modality\"] == \"video\":\n",
    "            stim_dur_ms = 200\n",
    "            stim_shape = (1, 3, 5, config[\"stim_size\"], config[\"stim_size\"])\n",
    "        elif config[\"modality\"] == \"image\":\n",
    "            stim_dur_ms = 120\n",
    "            stim_shape = (1, 3, config[\"stim_size\"], config[\"stim_size\"])\n",
    "        \n",
    "        # training parameters \n",
    "        config[\"exp_var_thresholds\"] = [0.1, 0.1, 0.1] #[-1, -1, -1] #[0.15, 0.15, 0.15, 0.15]\n",
    "        config[\"lr\"] = 0.001 #1 #[0.001, 0.01, 0.1]\n",
    "        config[\"batch_size\"] = 16\n",
    "        config[\"num_epochs\"] = 20 #1\n",
    "        config[\"l2_weight\"] = 0 #.001 #0.001 #0.001 # 0 #.001 #1#[0.01, 0.1, 1, 1]\n",
    "    \n",
    "        config[\"feat_ext_type\"] = 'resnet3d'\n",
    "        \n",
    "    # logging\n",
    "    config[\"wandb\"] = True\n",
    "    \n",
    "    # save model\n",
    "    config[\"save\"] = True\n",
    "    \n",
    "    # device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # session names\n",
    "    if config[\"modality\"] == \"video\":\n",
    "        session_ids = [\"082824\", \"082924\", \"083024\"] #\"082824\", \n",
    "    elif config[\"modality\"] == \"image\":\n",
    "        session_ids = [\"051724\", \"081624\", \"080124\", \"082324\"]\n",
    "    # image experiments\n",
    "    if config[\"modality\"] == \"video\":\n",
    "        feat_ext_types = [\"resnet3d\"] #[\"dorsalnet\", \"resnet3d\"]\n",
    "        layer_pool_sizes = [8, 4, 2, 1]\n",
    "    \n",
    "    elif config[\"modality\"] == \"image\":\n",
    "        feat_ext_types = [\"resnet50\", \"resnet50_fcn\"]\n",
    "        layer_pool_sizes = [4, 2, 1, 1]\n",
    "    \n",
    "    # video experiments\n",
    "    layers = ['layer3'] #['layer1', 'layer2', 'layer3', 'layer4']\n",
    "    \n",
    "    for layer, layer_pool_size in zip(layers, layer_pool_sizes):\n",
    "        config[\"layer\"] = layer\n",
    "        config[\"pool_size\"] = layer_pool_size\n",
    "        for feat_ext in feat_ext_types:\n",
    "            config[\"feat_ext_type\"] = feat_ext\n",
    "            \"\"\"\n",
    "corr_avgs = []\n",
    "for ses_idx, session_id in enumerate(session_ids):\n",
    "    sess_corr_avg = 0\n",
    "    sess_corrs = []\n",
    "    # logging training\n",
    "    config[\"session_id\"] = session_id\n",
    "\n",
    "    if config[\"wandb\"]:\n",
    "        wandb.init(\n",
    "            project=f'{config[\"modality\"]}-basline',\n",
    "            config=config,\n",
    "        )\n",
    "        wandb.define_metric(\"corr_to_avg\", summary=\"max\")\n",
    "        wandb.define_metric(\"test_loss\", summary=\"min\")\n",
    "\n",
    "    exp_var_threshold = config[\"exp_var_thresholds\"][ses_idx]\n",
    "    train_dataset, test_dataset, train_loader, test_loader = get_datasets_and_loaders(input_dir, session_id, config[\"modality\"], exp_var_threshold, stim_dur_ms, config[\"stim_size\"], config[\"win_size\"], stimulus_dir, config[\"batch_size\"], config[\"first_frame_only\"], blur_sigma = config[\"blur_sigma\"], pos = config['pos'])\n",
    "\n",
    "    if config[\"ensemble\"]:\n",
    "        full_model = ModelEnsembler([FullModel(config[\"modality\"], \"layer\" + str(jjj), stim_shape, train_dataset, use_sigma = config['use_sigma'], center_readout=config['center_readout'], use_pool = config['use_pool'], pool_size = config['pool_size'], pool_stride = config[\"pool_stride\"], use_pretrained = config[\"use_pretrained\"], feat_ext_type = config[\"feat_ext_type\"],flatten_time = config[\"flatten_time\"], device=device, mlp=config[\"mlp\"]) for jjj in range(1,5)])\n",
    "    else:\n",
    "        full_model = FullModel(config[\"modality\"], config[\"layer\"], stim_shape, train_dataset, use_sigma = config['use_sigma'], center_readout=config['center_readout'], use_pool = config['use_pool'], pool_size = config['pool_size'], pool_stride = config[\"pool_stride\"], use_pretrained = config[\"use_pretrained\"], feat_ext_type = config[\"feat_ext_type\"],flatten_time = config[\"flatten_time\"], device=device, mlp=config[\"mlp\"])\n",
    "\n",
    "    \n",
    "    params_with_l2 = []\n",
    "    params_without_l2 = []\n",
    "    for name, param in full_model.named_parameters():\n",
    "        if 'mu' in name or 'sigma' in name:\n",
    "            params_without_l2.append(param)\n",
    "        else:\n",
    "            params_with_l2.append(param)\n",
    "            \n",
    "    optimizer = torch.optim.Adam([\n",
    "    {'params': params_with_l2, 'weight_decay': config['l2_weight']},  # Apply L2 regularization (weight decay)\n",
    "    {'params': params_without_l2, 'weight_decay': 0.0}  # No L2 regularization\n",
    "    ], lr=config[\"lr\"], weight_decay=config['l2_weight'])\n",
    "\n",
    "    if config['loss'] == 'poisson':\n",
    "        loss_func = PoissonNLLLoss(log_input=False, full=True)\n",
    "    elif config['loss'] == 'mse':\n",
    "        loss_func = MSELoss()\n",
    "        \n",
    "    for epochs in range(config[\"num_epochs\"]):\n",
    "        epoch_loss = 0\n",
    "        for i, (stimulus, targets) in (enumerate(train_loader)): \n",
    "            stimulus = stimulus.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds = full_model(stimulus)\n",
    "            # on 10/4/24 - changed l2 weight decay to be a part of Adam optimizer \n",
    "            loss = loss_func(preds, targets) # + config[\"l2_weight\"] * torch.mean((torch.sum(full_model.model[1].linear.weight ** 2, 1)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # printing corr to avg and loss metrics \n",
    "        with torch.no_grad():\n",
    "            corr_avg = corr_to_avg(full_model, test_loader, modality=config[\"modality\"], device=device)\n",
    "            test_loss = 0\n",
    "            for i, (stimulus, targets) in enumerate(test_loader):\n",
    "                stimulus = stimulus.to(device)\n",
    "                targets = targets.to(device)\n",
    "                preds = full_model(stimulus) \n",
    "                loss = loss_func(preds, targets)\n",
    "                test_loss += loss.item()\n",
    "        if config[\"wandb\"]:\n",
    "            wandb.log({\"corr_to_avg\": np.nanmean(corr_avg), \"train_loss\": epoch_loss / len(train_loader), \"test_loss\": test_loss / len(test_loader)})\n",
    "        if np.nanmean(corr_avg) > sess_corr_avg:\n",
    "            sess_corr_avg = np.nanmean(corr_avg)\n",
    "            sess_corrs = corr_avg\n",
    "        print('  epoch {} loss: {} corr: {}'.format(epochs + 1, epoch_loss / len(train_dataset), np.nanmean(corr_avg)))\n",
    "        print(f' num. neurons : {len(corr_avg)}')\n",
    "    if config[\"save\"]:\n",
    "        torch.save(full_model.state_dict(), f\"{model_output_path}_{session_id}.pickle\")\n",
    "    corr_avgs.append(sess_corrs)\n",
    "    if config[\"wandb\"]:\n",
    "        wandb.finish()\n",
    "\n",
    "if config[\"wandb\"]:\n",
    "    wandb.init(\n",
    "        project=f'{config[\"modality\"]}-basline',\n",
    "        config=config,\n",
    "        #name=f\"{feat_ext}_{layer}_{config[\"first_frame_only\",\n",
    "    )\n",
    "    for sess_corr in corr_avgs:\n",
    "        for corr in sess_corr:\n",
    "            wandb.log({\"corr\": corr})\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for jj, (stimulus, targets) in (enumerate(train_loader)): \n",
    "    for j in range(config['batch_size']):\n",
    "        fig, axs = plt.subplots(1, 5)\n",
    "        for i in range(5):\n",
    "            plt.sca(axs[i])\n",
    "            plt.imshow(np.mean(stimulus[j, :, i, :, :].cpu().detach().numpy(),0), cmap='Grays') #, cmap='Grays'\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "    if jj > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.hist(full_model.models[4].model[1].sigma.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(full_model.models[4].model[1].mu.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(full_model.model[1].mu[:, 0].detach().cpu().numpy(), full_model.model[1].mu[:, 1].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fix_models.feature_extractors import hierat_reg\n",
    "model = hierat_reg(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.ones((1,3,224,224), device=device)).reshaped_hidden_states[-2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HieraModel.from_pretrained(\"facebook/hiera-tiny-224-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fix_models.feature_extractors import sam2t_reg\n",
    "model = sam2t_reg(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._forward_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_graph_node_names(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ./baselines/sam2/sam2/configs/sam2.1/sam2.1_hiera_t.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr_avg\n",
    "with open(join(input_dir, f\"{session_id}.pickle\"), \"rb\") as f:\n",
    "    model_input = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ev = model_input['expvar'][model_input['expvar'] > 0]\n",
    "plt.hist(ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "print(np.mean(corr_avg[ev > threshold]))\n",
    "print(len(corr_avg[ev > threshold]))\n",
    "plt.hist(corr_avg[ev > threshold] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ev[ev > threshold], corr_avg[ev > threshold])\n",
    "plt.axis('square')\n",
    "plt.ylim([0, 0.9])\n",
    "plt.xlim([0.1, 1])\n",
    "plt.plot([0.1, 0.9], [0.1, 0.9])\n",
    "plt.xlabel(\"explainable variance\")\n",
    "plt.ylabel(\"correlation to average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Search for natural stimuli that highly activate the population of neurons or single neurons in the neuron_ids vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# additional configuration variables for searching for highly activating stimuli\n",
    "modality = config[\"modality\"]\n",
    "config[\"stim_input_dir\"] = f\"./data/{modality}/novel_{modality}_datasets/\"\n",
    "config[\"stim_output_dir\"] = f\"./data/{modality}/novel_{modality}_datasets_pred/\"\n",
    "config[\"n_stim\"] = 25 # number of most activating and least activating stimuli to return \n",
    "config[\"n_log\"] = 5\n",
    "config[\"pop_act\"] = False\n",
    "config[\"neuron_ids\"] = [0]\n",
    "\n",
    "if config[\"pop_act\"]:\n",
    "    config[\"neuron_ids\"] = [0]\n",
    "\n",
    "for ses_idx, session_id in enumerate(session_ids):\n",
    "    # load model from state dict + get targets etc. for variable sizing\n",
    "    exp_var_threshold = config[\"exp_var_thresholds\"][ses_idx]\n",
    "    train_dataset, test_dataset, train_loader, test_loader = get_datasets_and_loaders(input_dir, session_id, config[\"modality\"], exp_var_threshold, stim_dur_ms, config[\"stim_size\"], config[\"win_size\"], stimulus_dir, config[\"batch_size\"])\n",
    "    full_model = FullModel(config[\"modality\"], config[\"layer\"], stim_shape, train_dataset, use_sigma = config['use_sigma'], center_readout=config['center_readout'], use_pool = config['use_pool'], pool_size = config['pool_size'], pool_stride = config[\"pool_stride\"], use_pretrained = config[\"use_pretrained\"], feat_ext_type = config[\"feat_ext_type\"],flatten_time = config[\"flatten_time\"], device=device)\n",
    "    \n",
    "    full_model.load_state_dict(torch.load(f\"{model_output_path}_{session_id}.pickle\", weights_only=True))\n",
    "    \n",
    "    for i, (stimulus, targets) in (enumerate(train_loader)): \n",
    "        break\n",
    "\n",
    "    # get dataset and loader for novel videos to search over\n",
    "    search_dataset, search_loader = get_search_dataset_and_loader(config[\"stim_input_dir\"], config[\"modality\"], config[\"stim_size\"], config[\"win_size\"], config[\"batch_size\"])\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    all_preds = torch.zeros((len(search_dataset), targets.shape[1]))\n",
    "    all_names = []\n",
    "    for i, (stimulus, stim_names) in (enumerate(search_loader)): \n",
    "        stimulus = stimulus.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = full_model(stimulus)\n",
    "            all_preds[i*batch_size:i*batch_size+batch_size, :] = preds\n",
    "            all_names = all_names + list(stim_names)\n",
    "    all_names = np.array(all_names)\n",
    "    \n",
    "    source_dir = config[\"stim_input_dir\"]  \n",
    "    target_dir = config[\"stim_output_dir\"] \n",
    "    \n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    for neuron_idx in config[\"neuron_ids\"]:\n",
    "        if config[\"pop_act\"]:\n",
    "            act_vect = torch.mean((all_preds - torch.mean(all_preds, 0))/torch.std(all_preds, 0), 1)\n",
    "        else:\n",
    "            act_vect = all_preds[:, neuron_idx]\n",
    "            \n",
    "        least_activating_videos = all_names[torch.argsort(act_vect)[:config[\"n_stim\"]].numpy().astype(int)]\n",
    "        most_activating_videos = all_names[torch.argsort(act_vect)[-config[\"n_stim\"]:].numpy().astype(int)]\n",
    "\n",
    "        least_activating_log = least_activating_videos[:config[\"n_log\"]]\n",
    "        most_activating_log = most_activating_videos[-config[\"n_log\"]:]\n",
    "\n",
    "        # save a text file with the most and least activating videos + copy the video files to the target directory\n",
    "        if config[\"pop_act\"]:\n",
    "            output_txt_path = os.path.join(target_dir, f'video_activation_names_{session_id}.txt')\n",
    "        else:\n",
    "            output_txt_path = os.path.join(target_dir, f'video_activation_names_{session_id}_{neuron_idx}.txt')\n",
    "\n",
    "        with open(output_txt_path, 'w') as f:\n",
    "            for video in least_activating_videos:\n",
    "                f.write(f'Least activating: {video}\\n')\n",
    "                if not os.path.exists(os.path.join(target_dir, video)):\n",
    "                    shutil.copy(os.path.join(source_dir, video), target_dir)\n",
    "            \n",
    "            for video in most_activating_videos:\n",
    "                f.write(f'Most activating: {video}\\n')\n",
    "                if not os.path.exists(os.path.join(target_dir, video)):\n",
    "                    shutil.copy(os.path.join(source_dir, video), target_dir)\n",
    "                    \n",
    "        print(f\"video names saved to {output_txt_path} and videos copied to {target_dir} for session {session_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Use models to synthesize highly activating input stimuli for the population of neurons or single neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AffineTransformationModel(nn.Module):\n",
    "    def __init__(self, device, steps=5):\n",
    "        super(AffineTransformationModel, self).__init__()\n",
    "        self.steps = steps\n",
    "        # Initialize the affine parameters close to identity\n",
    "        # Theta is of shape [2, 3]\n",
    "        # For Identity, theta is [[1, 0, 0], [0, 1, 0]]\n",
    "        \n",
    "        theta_identity = torch.tensor([[1, 0, 0],\n",
    "                                       [0, 1, 0]], dtype=torch.float32, device=device)\n",
    "        # Define theta_total as a trainable parameter\n",
    "        self.theta_total = nn.Parameter(theta_identity.clone())\n",
    "\n",
    "    \n",
    "        \n",
    "    def forward(self, img):\n",
    "        N, C, H, W = img.size()\n",
    "        device = img.device\n",
    "        frames = []\n",
    "        theta_identity = torch.tensor([[1, 0, 0],\n",
    "                                       [0, 1, 0]], dtype=torch.float32, device=device)\n",
    "        # For each step, compute theta_i\n",
    "        for i in range(0, self.steps):\n",
    "            t_i = i / self.steps\n",
    "            theta_i = theta_identity + t_i * (self.theta_total - theta_identity)\n",
    "            theta_i = theta_i.unsqueeze(0)  # [1, 2, 3]\n",
    "            grid = F.affine_grid(theta_i, img.size(), align_corners=False)\n",
    "            output = F.grid_sample(img, grid, align_corners=False)\n",
    "            frames.append(output)\n",
    "        return frames\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\"\"\"\n",
    "# Load an example image\n",
    "image_path = 'data/image/stimuli/n01440764_1145.JPEG'  # Replace with your image path\n",
    "img = Image.open(image_path)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "img = transform(img).unsqueeze(0)  # Convert to [1, C, H, W] tensor\n",
    "\n",
    "# print(train_dataset[0][0][:, 0, :, :].shape)\n",
    "model = AffineTransformationModel(steps=5)\n",
    "frames = model(img)\n",
    "\n",
    "# Plot the frames\n",
    "plt.figure(figsize=(15, 5))\n",
    "for idx, frame in enumerate(frames):\n",
    "    plt.subplot(1, 5, idx + 1)\n",
    "    # Convert tensor to NumPy array and transpose dimensions for plotting\n",
    "    frame_img = frame[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "    # Clip values to valid range [0, 1]\n",
    "    frame_img = frame_img.clip(0, 1)\n",
    "    plt.imshow(frame_img)\n",
    "    plt.title(f\"Frame {idx + 1}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "\n",
    "# additional configuration variables for searching for highly activating stimuli\n",
    "config[\"stim_output_dir\"] = f\"./data/{modality}/{modality}_datasets_meis/\"\n",
    "config[\"n_stim\"] = 25 # number of most activating and least activating stimuli to return \n",
    "config[\"n_log\"] = 5\n",
    "config[\"pop_act\"] = False\n",
    "config[\"neuron_ids\"] = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "if config[\"pop_act\"]:\n",
    "    config[\"neuron_ids\"] = [0]\n",
    "\n",
    "config[\"mei_lr\"] = 0.1\n",
    "neuron_id = 15#27\n",
    "num_iter = 1000\n",
    "norm_weight = 0\n",
    "# 0 #300 #250 # 0.3 #0.1\n",
    "blur = True\n",
    "blur_mag = 0.75#0.35 #0.5#0.5\n",
    "\n",
    "temp_lambda = 1000\n",
    "space_lambda = 0\n",
    "\n",
    "\n",
    "def temporal_smoothness_loss(video):\n",
    "    # video shape: (batch, channels, frames, height, width)\n",
    "    loss = 0.0\n",
    "    for t in range(video.shape[2] - 1):\n",
    "        loss += F.mse_loss(video[:, :, t, :, :], video[:, :, t+1, :, :])\n",
    "    #loss += F.mse_loss(video[:, :, 0, :, :], video[:, :, -1, :, :])\n",
    "    return loss\n",
    "\n",
    "def total_variation_loss(video):\n",
    "    tv_loss = torch.sum(torch.abs(video[:, :, :, :, :-1] - video[:, :, :, :, 1:])) + \\\n",
    "              torch.sum(torch.abs(video[:, :, :, :-1, :] - video[:, :, :, 1:, :]))\n",
    "    return tv_loss\n",
    "\n",
    "start_img = train_dataset[1][0][:, 0, :, :]\n",
    "start_img = torch.zeros(start_img.shape, device=device, dtype=start_img.dtype)\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def create_grid_image(image_size=32, square_size=4, spacing=4):\n",
    "    \"\"\"\n",
    "    Creates a black and white image with a grid of white squares on a black background.\n",
    "\n",
    "    Args:\n",
    "        image_size (int): The height and width of the image.\n",
    "        square_size (int): The size of each white square.\n",
    "        spacing (int): The spacing between the squares.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The generated image tensor of shape [1, 1, image_size, image_size].\n",
    "    \"\"\"\n",
    "    # Initialize a black image\n",
    "    img = torch.zeros((3, image_size, image_size), dtype=torch.float32)\n",
    "    \n",
    "    # Place white squares on the black background\n",
    "    bord = int(square_size/2)\n",
    "    for y in range(0, image_size - square_size + 1, square_size + spacing):\n",
    "        for x in range(0, image_size - square_size + 1, square_size + spacing):\n",
    "            img[:,  bord+ y:bord+y+square_size, bord+x:bord+x+square_size] = 1.0\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "start_img = create_grid_image(image_size=32, square_size=4, spacing=4)\n",
    "\n",
    "\n",
    "siz = start_img.shape[-2]\n",
    "\n",
    "for ses_idx, session_id in enumerate(session_ids):\n",
    "    # load model from state dict + get targets etc. for variable sizing\n",
    "    exp_var_threshold = config[\"exp_var_thresholds\"][ses_idx]\n",
    "    train_dataset, test_dataset, train_loader, test_loader = get_datasets_and_loaders(input_dir, session_id, config[\"modality\"], exp_var_threshold, stim_dur_ms, config[\"stim_size\"], config[\"win_size\"], stimulus_dir, config[\"batch_size\"])\n",
    "    full_model = FullModel(config[\"modality\"], config[\"layer\"], stim_shape, train_dataset, use_sigma = config['use_sigma'], center_readout=config['center_readout'], use_pool = config['use_pool'], pool_size = config['pool_size'], pool_stride = config[\"pool_stride\"], use_pretrained = config[\"use_pretrained\"], feat_ext_type = config[\"feat_ext_type\"],flatten_time = config[\"flatten_time\"], device=device)\n",
    "    \n",
    "    full_model.load_state_dict(torch.load(f\"{model_output_path}_{session_id}.pickle\", weights_only=True))\n",
    "    \n",
    "    for i, (stimulus, targets) in (enumerate(train_loader)): \n",
    "        break\n",
    "        \n",
    "    for param in full_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for neuron_id in config[\"neuron_ids\"]:\n",
    "    \n",
    "        model = AffineTransformationModel(steps=5, device=device)\n",
    "        #frames = model(img)\n",
    "    \n",
    "        #full_model.model[0].stim = nn.Parameter(start_img.to(device), requires_grad = True)\n",
    "    \n",
    "        optim = torch.optim.SGD(model.parameters(), lr=config[\"mei_lr\"], momentum=0.1)\n",
    "        \n",
    "        losses = []\n",
    "        for i in trange(num_iter):\n",
    "            # start img \n",
    "            frames = model(start_img.unsqueeze(0).to(device))\n",
    "            mod_input = torch.stack(frames, 0).permute(1, 2, 0, 3, 4)\n",
    "            loss = -full_model(mod_input)[0, neuron_id]\n",
    "            #loss = loss + norm_weight*torch.mean((full_model.model[0].stim - 0) ** 2) + \\\n",
    "            #temp_lambda * temporal_smoothness_loss(full_model.model[0].stim) + space_lambda * total_variation_loss(full_model.model[0].stim)\n",
    "            loss.backward() # backward pass\n",
    "            losses.append(loss.data.detach().cpu().numpy())\n",
    "     \n",
    "            optim.step() #gradient descent\n",
    "            optim.zero_grad()\n",
    "        \n",
    "            # gaussian blur over space\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                if blur:\n",
    "                    if config[\"modality\"] == \"video\":\n",
    "                        for j in range(stim_dims[2]):\n",
    "                            blur_trans = transforms.GaussianBlur(3, sigma=blur_mag)\n",
    "                            full_model.model[0].stim.data[0, :, j] = blur_trans(full_model.model[0].stim.data[0, :, j])\n",
    "                    else:\n",
    "                        blur_trans = transforms.GaussianBlur(3, sigma=blur_mag)\n",
    "                        full_model.model[0].stim.data[0] = blur_trans(full_model.model[0].stim.data[0].mean(0, keepdim=True)).repeat(3, 1, 1)\n",
    "                        #full_model[0].stim.data[0] = blur_trans(full_model[0].stim.data[0])\n",
    "            \"\"\"\n",
    "        stim = (full_model.model[0].stim.data.detach().cpu().numpy())\n",
    "\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(3, 3))\n",
    "        plt.plot(np.squeeze(np.array(losses))[10:])\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"synthetic neuron activity\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        fig, axs = plt.subplots(1, 5, figsize=(10, 2))\n",
    "        for i in range(5):\n",
    "            plt.sca(axs[i])\n",
    "            plt.imshow(np.mean(mod_input.detach().cpu().numpy(), (0,1))[i], cmap='Greys_r')\n",
    "        \n",
    "            #plt.clim([0, 1])\n",
    "            #plt.colorbar()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize=(10, 2))\n",
    "for i in range(5):\n",
    "    plt.sca(axs[i])\n",
    "    plt.imshow(np.mean(mod_input.detach().cpu().numpy(), (0,1))[i], cmap='Greys_r')\n",
    "\n",
    "    #plt.clim([0, 1])\n",
    "    #plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# additional configuration variables for searching for highly activating stimuli\n",
    "config[\"stim_output_dir\"] = f\"./data/{modality}/{modality}_datasets_meis/\"\n",
    "config[\"n_stim\"] = 25 # number of most activating and least activating stimuli to return \n",
    "config[\"n_log\"] = 5\n",
    "config[\"pop_act\"] = False\n",
    "config[\"neuron_ids\"] = [0]\n",
    "\n",
    "if config[\"pop_act\"]:\n",
    "    config[\"neuron_ids\"] = [0]\n",
    "\n",
    "config[\"mei_lr\"] = 0.5 \n",
    "neuron_id = 15#27\n",
    "num_iter = 1000\n",
    "norm_weight = 0\n",
    "# 0 #300 #250 # 0.3 #0.1\n",
    "blur = True\n",
    "blur_mag = 0.75#0.35 #0.5#0.5\n",
    "\n",
    "temp_lambda = 1000\n",
    "space_lambda = 0\n",
    "\n",
    "neuron_id = config[\"neuron_ids\"][0]\n",
    "\n",
    "def temporal_smoothness_loss(video):\n",
    "    # video shape: (batch, channels, frames, height, width)\n",
    "    loss = 0.0\n",
    "    for t in range(video.shape[2] - 1):\n",
    "        loss += F.mse_loss(video[:, :, t, :, :], video[:, :, t+1, :, :])\n",
    "    #loss += F.mse_loss(video[:, :, 0, :, :], video[:, :, -1, :, :])\n",
    "    return loss\n",
    "\n",
    "def total_variation_loss(video):\n",
    "    tv_loss = torch.sum(torch.abs(video[:, :, :, :, :-1] - video[:, :, :, :, 1:])) + \\\n",
    "              torch.sum(torch.abs(video[:, :, :, :-1, :] - video[:, :, :, 1:, :]))\n",
    "    return tv_loss\n",
    "\n",
    "start_img = train_dataset[0][0][:, 0, :, :]\n",
    "\n",
    "for ses_idx, session_id in enumerate(session_ids):\n",
    "    # load model from state dict + get targets etc. for variable sizing\n",
    "    exp_var_threshold = config[\"exp_var_thresholds\"][ses_idx]\n",
    "    train_dataset, test_dataset, train_loader, test_loader = get_datasets_and_loaders(input_dir, session_id, config[\"modality\"], exp_var_threshold, stim_dur_ms, config[\"stim_size\"], config[\"win_size\"], stimulus_dir, config[\"batch_size\"])\n",
    "    full_model = FullModel(config[\"modality\"], config[\"layer\"], stim_shape, train_dataset, use_sigma = config['use_sigma'], center_readout=config['center_readout'], use_pool = config['use_pool'], pool_size = config['pool_size'], pool_stride = config[\"pool_stride\"], use_pretrained = config[\"use_pretrained\"], feat_ext_type = config[\"feat_ext_type\"],flatten_time = config[\"flatten_time\"], device=device)\n",
    "    \n",
    "    full_model.load_state_dict(torch.load(f\"{model_output_path}_{session_id}.pickle\", weights_only=True))\n",
    "    \n",
    "    for i, (stimulus, targets) in (enumerate(train_loader)): \n",
    "        break\n",
    "        \n",
    "    for param in full_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    \n",
    "    full_model.model[0].stim.data = full_model.model[0].stim.data.to(device) - 0.5\n",
    "    init_stim = deepcopy(full_model.model[0].stim.data.detach().cpu().numpy())\n",
    "\n",
    "    full_model.model[0].stim.requires_grad=True\n",
    "    optim = torch.optim.SGD([full_model.model[0].stim], lr=config[\"mei_lr\"], momentum=0.1)\n",
    "    \n",
    "    losses = []\n",
    "    for i in trange(num_iter):\n",
    "        loss = -full_model(full_model.model[0].stim)[0, neuron_id]\n",
    "        loss = loss + norm_weight*torch.mean((full_model.model[0].stim - 0) ** 2) + \\\n",
    "        temp_lambda * temporal_smoothness_loss(full_model.model[0].stim) + space_lambda * total_variation_loss(full_model.model[0].stim)\n",
    "        loss.backward() # backward pass\n",
    "        losses.append(loss.data.detach().cpu().numpy())\n",
    " \n",
    "        optim.step() #gradient descent\n",
    "        optim.zero_grad()\n",
    "    \n",
    "        # gaussian blur over space\n",
    "        with torch.no_grad():\n",
    "            if blur:\n",
    "                if config[\"modality\"] == \"video\":\n",
    "                    for j in range(stim_dims[2]):\n",
    "                        blur_trans = transforms.GaussianBlur(3, sigma=blur_mag)\n",
    "                        full_model.model[0].stim.data[0, :, j] = blur_trans(full_model.model[0].stim.data[0, :, j])\n",
    "                else:\n",
    "                    blur_trans = transforms.GaussianBlur(3, sigma=blur_mag)\n",
    "                    full_model.model[0].stim.data[0] = blur_trans(full_model.model[0].stim.data[0].mean(0, keepdim=True)).repeat(3, 1, 1)\n",
    "                    #full_model[0].stim.data[0] = blur_trans(full_model[0].stim.data[0])\n",
    "    \n",
    "    stim = (full_model.model[0].stim.data.detach().cpu().numpy())\n",
    "    \n",
    "    plt.plot(np.squeeze(np.array(losses))[10:])\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"synthetic neuron activity\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"modality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.imshow(np.mean(stim, (0,1))[i], cmap='Greys_r')\n",
    "\n",
    "    #plt.clim([0, 1])\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    plt.imshow(np.mean(stim, (0,1))[i] - np.mean(stim, (0,1))[i+1], cmap='Greys_r')\n",
    "\n",
    "    #plt.clim([0.3, 0.7])\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - in silico electrophysiology experiments/rf centers/tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRIFTING GRATINGS\n",
    "def create_drifting_gratings():\n",
    "    ntau = 5\n",
    "    radius = 16\n",
    "    ndirections = 8\n",
    "\n",
    "    lx, lt = 16, 16\n",
    "\n",
    "    # Create stimuli that contain all combos that are needed\n",
    "    xi, yi = np.meshgrid(np.arange(-16, 16), np.arange(-16, 16))\n",
    "    mask = xi**2 + yi**2 < radius**2\n",
    "    oi = (np.arange(ndirections) / ndirections * 2 * np.pi).reshape((-1, 1, 1, 1))\n",
    "    ti = np.arange(ntau)\n",
    "    ti = ti - ti.mean()\n",
    "\n",
    "    vals = []\n",
    "    stims = []\n",
    "\n",
    "    ri = (np.cos(oi) * xi.reshape((1, 1, xi.shape[0], xi.shape[1])) - np.sin(oi) * yi.reshape((1, 1, xi.shape[0], xi.shape[1])))\n",
    "    X = mask.reshape((1, 1, xi.shape[0], xi.shape[1])) * np.cos((ri / lx) * 2 * np.pi - ti.reshape((1, -1, 1, 1)) / lt * 2 *np.pi)\n",
    "    X = np.stack([X, X, X], axis=1) # Go from black and white to RGB\n",
    "    return X\n",
    "\n",
    "X_drift = torch.tensor(create_drifting_gratings()).to(device='cuda', dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPIRAL SQUARE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Function to generate a single frame\n",
    "def create_square_frame(size, rotation, canvas_size=32):\n",
    "    img = Image.new('L', (canvas_size, canvas_size), color=0)  # Create black canvas\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Define the square's coordinates (centered)\n",
    "    top_left = ((canvas_size - size) // 2, (canvas_size - size) // 2)\n",
    "    bottom_right = ((canvas_size + size) // 2, (canvas_size + size) // 2)\n",
    "    \n",
    "    # Draw the square\n",
    "    draw.rectangle([top_left, bottom_right], fill=255)\n",
    "    \n",
    "    # Rotate the image\n",
    "    img = img.rotate(rotation, expand=False)\n",
    "    \n",
    "    return np.array(img)\n",
    "\n",
    "# Function to generate frames for a specific transformation\n",
    "def generate_stimuli_frames(expanding=True, rotating_clockwise=True, num_frames=5):\n",
    "    frames = []\n",
    "    \n",
    "    if expanding is True:\n",
    "        sizes = np.linspace(10, 20, num_frames)  # Square grows in size\n",
    "    elif expanding is False:\n",
    "        sizes = np.linspace(20, 10, num_frames)  # Square shrinks in size\n",
    "    else:\n",
    "        sizes = [15] * num_frames  # Keep square the same size if no expansion/contraction\n",
    "    \n",
    "    if rotating_clockwise is True:\n",
    "        rotations = np.linspace(0, -45, num_frames)  # Clockwise rotation\n",
    "    elif rotating_clockwise is False:\n",
    "        rotations = np.linspace(0, 45, num_frames)  # Counterclockwise rotation\n",
    "    else:\n",
    "        rotations = [0] * num_frames  # No rotation\n",
    "    \n",
    "    for size, rotation in zip(sizes, rotations):\n",
    "        frame = gaussian_filter(create_square_frame(int(size), rotation), sigma=2)\n",
    "        frames.append(frame)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "# Function to visualize frames using matplotlib\n",
    "def visualize_stimuli_frames(frames):\n",
    "    fig, axes = plt.subplots(1, len(frames), figsize=(15, 5))\n",
    "    \n",
    "    # Plot each frame\n",
    "    for i, frame in enumerate(frames):\n",
    "        axes[i].imshow(frame, cmap='gray')\n",
    "        axes[i].axis('off')  # Hide the axes\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Generate and visualize 8 different stimuli for each condition\n",
    "conditions = [\n",
    "    {\"expanding\": True, \"rotating_clockwise\": None},  # Only expanding\n",
    "    {\"expanding\": True, \"rotating_clockwise\": True},\n",
    "    {\"expanding\": None, \"rotating_clockwise\": True},   # Only rotating clockwise\n",
    "    {\"expanding\": False, \"rotating_clockwise\": True},\n",
    "    {\"expanding\": False, \"rotating_clockwise\": None},  # Only contracting\n",
    "    {\"expanding\": False, \"rotating_clockwise\": False},\n",
    "    {\"expanding\": None, \"rotating_clockwise\": False},   # Only rotating counterclockwise\n",
    "    {\"expanding\": True, \"rotating_clockwise\": False},\n",
    "]\n",
    "    \n",
    "# Visualize 8 stimuli for the first condition as an example\n",
    "all_stim = []\n",
    "for j in range(8):  # 8 stimuli per condition\n",
    "    condition = conditions[j]  # Change the index for other conditions\n",
    "    expanding = condition[\"expanding\"] if condition[\"expanding\"] is not None else None\n",
    "    rotating_clockwise = condition[\"rotating_clockwise\"] if condition[\"rotating_clockwise\"] is not None else None\n",
    "    \n",
    "    frames = generate_stimuli_frames(expanding=expanding, rotating_clockwise=rotating_clockwise)\n",
    "    all_stim.append(frames)\n",
    "    visualize_stimuli_frames(frames)\n",
    "\n",
    "X_spiral = torch.tensor(np.array(all_stim), dtype=torch.float32).unsqueeze(1).repeat(1, 3, 1, 1, 1).to(device)/255/2+0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_predictions(X):\n",
    "    config[\"neuron_ids\"] = [0,1]\n",
    "    all_preds = []\n",
    "    \n",
    "    for ses_idx, session_id in enumerate(session_ids):\n",
    "        # load model from state dict + get targets etc. for variable sizing\n",
    "        exp_var_threshold = config[\"exp_var_thresholds\"][ses_idx]\n",
    "    \n",
    "        train_dataset, test_dataset, train_loader, test_loader = get_datasets_and_loaders(input_dir, session_id, config[\"modality\"], exp_var_threshold, stim_dur_ms, config[\"stim_size\"], config[\"win_size\"], stimulus_dir, config[\"batch_size\"])\n",
    "        full_model = FullModel(config[\"modality\"], config[\"layer\"], stim_shape, train_dataset, use_sigma = config['use_sigma'], center_readout=config['center_readout'], use_pool = config['use_pool'], pool_size = config['pool_size'], pool_stride = config[\"pool_stride\"], use_pretrained = config[\"use_pretrained\"], feat_ext_type = config[\"feat_ext_type\"],flatten_time = config[\"flatten_time\"], device=device)\n",
    "    \n",
    "        full_model.load_state_dict(torch.load(f\"{model_output_path}_{session_id}.pickle\", weights_only=True))\n",
    "        \n",
    "        for i, (stimulus, targets) in (enumerate(train_loader)): \n",
    "            break\n",
    "            \n",
    "        for param in full_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "        preds = full_model(X)\n",
    "        all_preds.append(preds.detach().cpu().numpy())\n",
    "    \n",
    "    preds = np.column_stack(all_preds)\n",
    "    return preds\n",
    "\n",
    "def get_rf_preds():\n",
    "    all_preds = []\n",
    "    \n",
    "    for ses_idx, session_id in enumerate(session_ids):\n",
    "        # load model from state dict + get targets etc. for variable sizing\n",
    "        exp_var_threshold = config[\"exp_var_thresholds\"][ses_idx]\n",
    "    \n",
    "        train_dataset, test_dataset, train_loader, test_loader = get_datasets_and_loaders(input_dir, session_id, config[\"modality\"], exp_var_threshold, stim_dur_ms, config[\"stim_size\"], config[\"win_size\"], stimulus_dir, config[\"batch_size\"])\n",
    "        full_model = FullModel(config[\"modality\"], config[\"layer\"], stim_shape, train_dataset, use_sigma = config['use_sigma'], center_readout=config['center_readout'], use_pool = config['use_pool'], pool_size = config['pool_size'], pool_stride = config[\"pool_stride\"], use_pretrained = config[\"use_pretrained\"], feat_ext_type = config[\"feat_ext_type\"],flatten_time = config[\"flatten_time\"], device=device)\n",
    "    \n",
    "        full_model.load_state_dict(torch.load(f\"{model_output_path}_{session_id}.pickle\", weights_only=True))\n",
    "        \n",
    "        for i, (stimulus, targets) in (enumerate(train_loader)): \n",
    "            break\n",
    "            \n",
    "        for param in full_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for i in range(full_model.model[1].mu.shape[0]):\n",
    "            all_preds.append(full_model.model[1].mu[i, :].detach().cpu().numpy())\n",
    "    \n",
    "    preds = np.column_stack(all_preds)\n",
    "    return preds\n",
    "    \n",
    "spiral_preds = get_all_predictions(X_spiral)\n",
    "grating_preds = get_all_predictions(X_drift)\n",
    "rf_preds = get_rf_preds()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true values of spiral tuning, drift tuning, and RF location\n",
    "spiral_true = []\n",
    "grating_true = []\n",
    "rf_true = []\n",
    "expvars = []\n",
    "for session_id in session_ids: \n",
    "    spiral_tuning = np.load(f\"./data/spiral_selectivity/{session_id}.npy\").T\n",
    "    dir_tuning = np.load(f\"./data/dir_selectivity/{session_id}.npy\").T\n",
    "    rf_maps = np.load(f\"./data/rf_maps/{session_id}.npy\")\n",
    "\n",
    "    input_dir = f'./data/{config[\"modality\"]}/'\n",
    "    thresh = config[\"exp_var_thresholds\"][ses_idx]\n",
    "    with open(join(input_dir, f\"{session_id}.pickle\"), \"rb\") as f:\n",
    "        model_input = pickle.load(f)\n",
    "    \n",
    "    spiral_true.append(spiral_tuning[:, model_input['expvar'] > thresh])\n",
    "    grating_true.append(dir_tuning[:, model_input['expvar'] > thresh])\n",
    "    rf_maps = rf_maps[model_input['expvar'] > thresh]\n",
    "    rf_pos = np.array([np.unravel_index(np.argmax(rf_maps[i, :, :]), rf_maps[i, :, :].shape) for i in range(rf_maps.shape[0])])\n",
    "    plt.scatter(rf_pos[:, 0], rf_pos[:, 1], alpha=0.1)\n",
    "    plt.show()\n",
    "    rf_true.extend([np.unravel_index(np.argmax(rf_maps[i, :, :]), rf_maps[i, :, :].shape) for i in range(rf_maps.shape[0])])\n",
    "    expvars.append(model_input['expvar'][model_input['expvar'] > thresh][None, :])\n",
    "spiral_true = np.column_stack(spiral_true)\n",
    "grating_true = np.column_stack(grating_true)\n",
    "rf_true = np.array(rf_true).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def get_pearsonr(true, preds, jj = 0):\n",
    "    rs = []\n",
    "    ps = []\n",
    "    for i in range(preds.shape[1]):\n",
    "        r, p = pearsonr(true[:, i], np.roll(preds[:, i], jj))\n",
    "        rs.append(r)\n",
    "        ps.append(p)\n",
    "    return np.array(rs), np.array(ps)\n",
    "\n",
    "def get_pearsonr_rf(true, preds):\n",
    "    r1 = pearsonr(-true[0, :], preds[0, :]).statistic\n",
    "    r2 = pearsonr(-true[1, :], preds[1, :]).statistic\n",
    "    return r2\n",
    "\n",
    "spiral_rs, spiral_ps = get_pearsonr(spiral_true, spiral_preds)\n",
    "grating_rs, grating_ps = get_pearsonr(grating_true, grating_preds, 4)\n",
    "rf_rs = get_pearsonr_rf(rf_true, rf_preds)\n",
    "\n",
    "print(f\"spiral correlation {np.nanmean(spiral_rs)}\")\n",
    "print(f\"grating correlation {np.nanmean(grating_rs)}\")\n",
    "print(f\"rf correlation {np.nanmean(rf_rs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.grid_sample(torch.tensor([[[[0,0,0,1],[0,0,0,0],[0,0,0,0]]]], dtype=torch.float32), torch.tensor([[[[1, 1]]]], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[[[0,0,0,1],[0,0,0,0],[0,0,0,0]]]], dtype=torch.float32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(rf_preds[0, :], rf_preds[1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(rf_true[0, :], rf_true[1, :], alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(grating_ps < 0.05)/len(grating_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(spiral_ps < 0.05)/len(grating_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(x):\n",
    "    x = x - np.min(x)\n",
    "    x = x/np.max(x)\n",
    "    return x\n",
    "\n",
    "for i in range(grating_ps.shape[0]):\n",
    "    plt.plot(rescale(grating_true[:, i]))\n",
    "    plt.plot(rescale(grating_preds[:, i]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiral_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.array(rs)\n",
    "rs[np.isnan(rs)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(rs))\n",
    "np.max(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expvars[:, 546]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.argmax(rs)\n",
    "XX = all_spiral[:, i]\n",
    "plt.plot(XX)\n",
    "plt.show()\n",
    "\n",
    "XX = preds[:, i].detach().cpu().numpy()\n",
    "plt.plot(XX)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830e1050-a934-43ae-b4f0-9a5ca0517603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "from torch.nn import PoissonNLLLoss\n",
    "from fix_models.metrics import corr_to_avg\n",
    "\n",
    "from fix_models.datasets import get_datasets_and_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140b03c3-58b7-4b98-84a1-15e444a45001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "# all parameters\n",
    "config = dict()\n",
    "config[\"modality\"] = \"video\" # or image\n",
    "\n",
    "# paths\n",
    "input_dir = f'../data/{config[\"modality\"]}/'\n",
    "stimulus_dir = f'../data/{config[\"modality\"]}/stimuli/'\n",
    "embedding_dir = f'../data/{config[\"modality\"]}/embeddings/'\n",
    "model_output_path = f'../data/{config[\"modality\"]}/model_output/results'\n",
    "\n",
    "# dataset and dataloader hyperparameters \n",
    "config[\"win_size\"] = 240\n",
    "config['pos'] = (400, 180)\n",
    "config[\"feat_ext_type\"] = 'resnet3d'\n",
    "config[\"stim_size\"] = 32 \n",
    "config[\"stim_dur_ms\"] = 200\n",
    "config[\"stim_shape\"] = (1, 3, 5, config[\"stim_size\"], config[\"stim_size\"])\n",
    "config[\"first_frame_only\"] = False\n",
    "config[\"exp_var_thresholds\"] = [0.25, 0.25, 0.25]\n",
    "config[\"batch_size\"] = 16\n",
    "\n",
    "# model hyperparameters\n",
    "config[\"layer\"] = \"layer1\"\n",
    "config[\"use_sigma\"] = True\n",
    "config[\"center_readout\"] = False\n",
    "config[\"use_pool\"] = True\n",
    "config[\"pool_size\"] = 4\n",
    "config[\"pool_stride\"] = 2\n",
    "config[\"use_pretrained\"] = True\n",
    "config[\"flatten_time\"] = True\n",
    "\n",
    "# training parameters \n",
    "config[\"lr\"] = 0.001 \n",
    "config[\"num_epochs\"] = 40\n",
    "config[\"l2_weight\"] = 0\n",
    "\n",
    "# logging\n",
    "config[\"wandb\"] = True\n",
    "\n",
    "# save model\n",
    "config[\"save\"] = True\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# session names\n",
    "session_ids = [\"082824\", \"082924\", \"083024\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec88115-3400-4df0-a713-b7094212edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(full_model_fcn, model_name):\n",
    "    # corr avgs\n",
    "    corr_avgs = []\n",
    "\n",
    "    config['model_name'] = model_name\n",
    "\n",
    "    print(config['l2_weight'])\n",
    "    \n",
    "    for ses_idx, session_id in enumerate(session_ids):\n",
    "        # set sess_corr_avg\n",
    "        sess_corr_avg = -1\n",
    "        sess_corrs = []\n",
    "\n",
    "        # set session index \n",
    "        config[\"session_id\"] = session_id\n",
    "\n",
    "        # setup logging\n",
    "        if config[\"wandb\"]:\n",
    "            wandb.init(\n",
    "                project=f'{config[\"modality\"]}-cs230',\n",
    "                config=config,\n",
    "            )\n",
    "            wandb.define_metric(\"corr_to_avg\", summary=\"max\")\n",
    "            wandb.define_metric(\"test_loss\", summary=\"min\")\n",
    "\n",
    "        # load datasets and loaders \n",
    "        train_dataset, test_dataset, train_loader, test_loader = get_datasets_and_loaders(input_dir, session_id, config[\"modality\"], config[\"exp_var_thresholds\"][ses_idx], config[\"stim_dur_ms\"], config[\"stim_size\"], config[\"win_size\"], stimulus_dir, config[\"batch_size\"], config[\"first_frame_only\"], pos = config['pos'])\n",
    "\n",
    "        full_model = full_model_fcn(train_dataset)\n",
    "\n",
    "        for name, param in full_model.named_parameters():\n",
    "            print(name)\n",
    "            print(param.shape)\n",
    "            print(param.requires_grad)\n",
    "        # set which parameters to use regularization with and which not to\n",
    "        params_with_l2 = []\n",
    "        params_without_l2 = []\n",
    "        for name, param in full_model.named_parameters():\n",
    "            if 'mu' in name or 'sigma' in name or 'bias' in name:\n",
    "                params_without_l2.append(param)\n",
    "            else:\n",
    "                params_with_l2.append(param)\n",
    "\n",
    "        # setup Adam optimizer\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': params_with_l2, 'weight_decay': config['l2_weight']},  # Apply L2 regularization (weight decay)\n",
    "            {'params': params_without_l2, 'weight_decay': 0.0}  # No L2 regularization\n",
    "        ], lr=config[\"lr\"], weight_decay=config['l2_weight'])\n",
    "    \n",
    "        # using poisson loss \n",
    "        loss_func = PoissonNLLLoss(log_input=False, full=True)\n",
    "            \n",
    "        for epochs in range(config[\"num_epochs\"]):\n",
    "            epoch_loss = 0\n",
    "            for i, (stimulus, targets) in enumerate(train_loader): \n",
    "                stimulus = stimulus.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                preds = full_model(stimulus)\n",
    "                loss = loss_func(preds, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "    \n",
    "            # printing corr to avg and loss metrics \n",
    "            with torch.no_grad():\n",
    "                corr_avg = corr_to_avg(full_model, test_loader, modality=config[\"modality\"], device=device)\n",
    "                test_loss = 0\n",
    "                for i, (stimulus, targets) in enumerate(test_loader):\n",
    "                    stimulus = stimulus.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    preds = full_model(stimulus) \n",
    "                    loss = loss_func(preds, targets)\n",
    "                    test_loss += loss.item()\n",
    "                    \n",
    "            if config[\"wandb\"]:\n",
    "                wandb.log({\"corr_to_avg\": np.nanmean(corr_avg), \"train_loss\": epoch_loss / len(train_loader), \"test_loss\": test_loss / len(test_loader)})\n",
    "            \n",
    "            if np.nanmean(corr_avg) > sess_corr_avg:\n",
    "                sess_corr_avg = np.nanmean(corr_avg)\n",
    "                sess_corrs = corr_avg\n",
    "                \n",
    "            print('  epoch {} loss: {} corr: {}'.format(epochs + 1, epoch_loss / len(train_dataset), np.nanmean(corr_avg)))\n",
    "            print(f' num. neurons : {len(corr_avg)}')\n",
    "            \n",
    "        if config[\"save\"]:\n",
    "            torch.save(full_model.state_dict(), f\"{model_output_path}_{session_id}.pickle\")\n",
    "            \n",
    "        corr_avgs.append(sess_corrs)\n",
    "        \n",
    "        if config[\"wandb\"]:\n",
    "            wandb.finish()\n",
    "    \n",
    "    if config[\"wandb\"]:\n",
    "        wandb.init(\n",
    "            project=f'{config[\"modality\"]}-cs230',\n",
    "            config=config,\n",
    "        )\n",
    "        for sess_corr in corr_avgs:\n",
    "            for corr in sess_corr:\n",
    "                wandb.log({\"corr\": corr})\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "815b4bbe-5dfa-415e-b56e-045676e39232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#### 1) poisson GLM trained from scratch\\npoisson_glm_fcn = lambda train_dataset: FullModel(feat_ext_type = \\'none\\', freeze_weights=False, use_pretrained = False, modality=config[\"modality\"], layer=config[\"layer\"], stim_shape=config[\"stim_shape\"], train_dataset=train_dataset, use_pool = config[\\'use_pool\\'], pool_size = config[\\'pool_size\\'], pool_stride = config[\"pool_stride\"], device=device)\\n#### 2) frozen pretrained CNN with poisson GLM readout\\ncnn_frozen_fcn = lambda train_dataset:FullModel(feat_ext_type = \\'resnet3d\\', freeze_weights=True, use_pretrained = True, modality=config[\"modality\"], layer=config[\"layer\"], stim_shape=config[\"stim_shape\"], train_dataset=train_dataset, use_pool = config[\\'use_pool\\'], pool_size = config[\\'pool_size\\'], pool_stride = config[\"pool_stride\"], device=device)\\n#### 3) unfrozen pretrained CNN with poisson GLM readout\\ncnn_unfrozen_fcn = lambda train_dataset:FullModel(feat_ext_type = \\'resnet3d\\', freeze_weights=False, use_pretrained = True, modality=config[\"modality\"], layer=config[\"layer\"], stim_shape=config[\"stim_shape\"], train_dataset=train_dataset, use_pool = config[\\'use_pool\\'], pool_size = config[\\'pool_size\\'], pool_stride = config[\"pool_stride\"], device=device)\\n#### 4) CNN trained from scratch with poisson GLM readout\\ncnn_untrained_fcn = lambda train_dataset:FullModel(feat_ext_type = \\'resnet3d\\', freeze_weights=False, use_pretrained = False, modality=config[\"modality\"], layer=config[\"layer\"], stim_shape=config[\"stim_shape\"], train_dataset=train_dataset, use_pool = config[\\'use_pool\\'], pool_size = config[\\'pool_size\\'], pool_stride = config[\"pool_stride\"], device=device)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### train 4 encoding models: \n",
    "from fix_models.models import FullModel\n",
    "\n",
    "\"\"\"\n",
    "#### 1) poisson GLM trained from scratch\n",
    "poisson_glm_fcn = lambda train_dataset: FullModel(feat_ext_type = 'none', freeze_weights=False, use_pretrained = False, modality=config[\"modality\"], layer=config[\"layer\"], stim_shape=config[\"stim_shape\"], train_dataset=train_dataset, use_pool = config['use_pool'], pool_size = config['pool_size'], pool_stride = config[\"pool_stride\"], device=device)\n",
    "#### 2) frozen pretrained CNN with poisson GLM readout\n",
    "cnn_frozen_fcn = lambda train_dataset:FullModel(feat_ext_type = 'resnet3d', freeze_weights=True, use_pretrained = True, modality=config[\"modality\"], layer=config[\"layer\"], stim_shape=config[\"stim_shape\"], train_dataset=train_dataset, use_pool = config['use_pool'], pool_size = config['pool_size'], pool_stride = config[\"pool_stride\"], device=device)\n",
    "#### 3) unfrozen pretrained CNN with poisson GLM readout\n",
    "cnn_unfrozen_fcn = lambda train_dataset:FullModel(feat_ext_type = 'resnet3d', freeze_weights=False, use_pretrained = True, modality=config[\"modality\"], layer=config[\"layer\"], stim_shape=config[\"stim_shape\"], train_dataset=train_dataset, use_pool = config['use_pool'], pool_size = config['pool_size'], pool_stride = config[\"pool_stride\"], device=device)\n",
    "#### 4) CNN trained from scratch with poisson GLM readout\n",
    "cnn_untrained_fcn = lambda train_dataset:FullModel(feat_ext_type = 'resnet3d', freeze_weights=False, use_pretrained = False, modality=config[\"modality\"], layer=config[\"layer\"], stim_shape=config[\"stim_shape\"], train_dataset=train_dataset, use_pool = config['use_pool'], pool_size = config['pool_size'], pool_stride = config[\"pool_stride\"], device=device)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e6c805d-85a1-4715-93e7-79167c24ae5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: layer1\n",
      "0\n",
      "readout input shape: 320\n",
      "model.0.stim\n",
      "torch.Size([1, 3, 5, 32, 32])\n",
      "True\n",
      "model.1.mu\n",
      "torch.Size([54, 2])\n",
      "True\n",
      "model.1.sigma\n",
      "torch.Size([54])\n",
      "True\n",
      "model.1.poisson_linear.linear.weight\n",
      "torch.Size([54, 320])\n",
      "True\n",
      "model.1.poisson_linear.linear.bias\n",
      "torch.Size([54])\n",
      "True\n",
      "  epoch 1 loss: 0.11927233177938579 corr: 0.10107337113395437\n",
      " num. neurons : 54\n",
      "  epoch 2 loss: 0.11063482060844515 corr: 0.12282421543545964\n",
      " num. neurons : 54\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#### 4) poisson GLM trained from scratch\u001b[39;00m\n\u001b[1;32m     13\u001b[0m poisson_glm_fcn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m train_dataset: FullModel(feat_ext_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, freeze_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_pretrained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, modality\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodality\u001b[39m\u001b[38;5;124m\"\u001b[39m], layer\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m\"\u001b[39m], stim_shape\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstim_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m], train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset, use_pool \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_pool\u001b[39m\u001b[38;5;124m'\u001b[39m], pool_size \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpool_size\u001b[39m\u001b[38;5;124m'\u001b[39m], pool_stride \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpool_stride\u001b[39m\u001b[38;5;124m\"\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_frozen_fcn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrozen pretrained\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m train_model(cnn_unfrozen_fcn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munfrozen pretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m train_model(cnn_untrained_fcn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muntrained\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 55\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(full_model_fcn, model_name)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epochs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m     54\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (stimulus, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader): \n\u001b[1;32m     56\u001b[0m         stimulus \u001b[38;5;241m=\u001b[39m stimulus\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     57\u001b[0m         targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for layer in ['layer1', 'layer2', 'layer3', 'layer4']:\n",
    "    config['layer'] = layer\n",
    "    print(f'layer: {layer}')\n",
    "\n",
    "    #### 1) frozen pretrained CNN with poisson GLM readout\n",
    "    cnn_frozen_fcn = lambda train_dataset:FullModel(feat_ext_type = 'resnet3d', freeze_weights=True, use_pretrained = True, modality=config[\"modality\"], layer=config[\"layer\"], stim_shape=config[\"stim_shape\"], train_dataset=train_dataset, use_pool = config['use_pool'], pool_size = config['pool_size'], pool_stride = config[\"pool_stride\"], device=device)\n",
    "    #### 2) unfrozen pretrained CNN with poisson GLM readout\n",
    "    cnn_unfrozen_fcn = lambda train_dataset:FullModel(feat_ext_type = 'resnet3d', freeze_weights=False, use_pretrained = True, modality=config[\"modality\"], layer=config[\"layer\"], stim_shape=config[\"stim_shape\"], train_dataset=train_dataset, use_pool = config['use_pool'], pool_size = config['pool_size'], pool_stride = config[\"pool_stride\"], device=device)\n",
    "    #### 3) CNN trained from scratch with poisson GLM readout\n",
    "    cnn_untrained_fcn = lambda train_dataset:FullModel(feat_ext_type = 'resnet3d', freeze_weights=False, use_pretrained = False, modality=config[\"modality\"], layer=config[\"layer\"], stim_shape=config[\"stim_shape\"], train_dataset=train_dataset, use_pool = config['use_pool'], pool_size = config['pool_size'], pool_stride = config[\"pool_stride\"], device=device)\n",
    "    #### 4) poisson GLM trained from scratch\n",
    "    poisson_glm_fcn = lambda train_dataset: FullModel(feat_ext_type = 'none', freeze_weights=False, use_pretrained = False, modality=config[\"modality\"], layer=config[\"layer\"], stim_shape=config[\"stim_shape\"], train_dataset=train_dataset, use_pool = config['use_pool'], pool_size = config['pool_size'], pool_stride = config[\"pool_stride\"], device=device)\n",
    "\n",
    "    config['l2_weight'] = 0 #l2_weight\n",
    "    train_model(cnn_frozen_fcn, \"frozen pretrained\")\n",
    "    train_model(cnn_unfrozen_fcn, \"unfrozen pretrained\")\n",
    "    train_model(cnn_untrained_fcn, \"untrained\")\n",
    "    config['l2_weight'] = 100\n",
    "    train_model(poisson_glm_fcn, \"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5337a6e-2aea-4755-ba9c-3a3027561f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    train_model(cnn_frozen_fcn, \"frozen pretrained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a3a60-6741-4602-a88d-8469213a4746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
